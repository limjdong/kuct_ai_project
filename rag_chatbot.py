import streamlit as st
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import OpenAIEmbeddings
from langchain_community.chat_models import ChatOpenAI
from langchain_core.runnables import RunnableLambda
from langchain_core.prompts import ChatPromptTemplate
from dotenv import load_dotenv
import os
import re

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv("env.txt")

# ë²¡í„° ì €ì¥ì†Œ ë¡œë“œ (ìºì‹œ ì ìš©ìœ¼ë¡œ ì„±ëŠ¥ í–¥ìƒ)
@st.cache_resource
def load_vectorstore(vectorstore_path="vectorstore"):
    """ì‚¬ì „ ìƒì„±ëœ FAISS ë²¡í„° ì €ì¥ì†Œ ë¡œë“œ (ìºì‹œë¨)"""
    if not os.path.exists(vectorstore_path):
        raise FileNotFoundError(
            f"ë²¡í„° ì €ì¥ì†Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {vectorstore_path}\n"
            "ë¨¼ì € 'python create_embeddings.py'ë¥¼ ì‹¤í–‰í•˜ì—¬ ì„ë² ë”©ì„ ìƒì„±í•˜ì„¸ìš”."
        )
    embeddings = OpenAIEmbeddings()
    return FAISS.load_local(vectorstore_path, embeddings, allow_dangerous_deserialization=True)

# ì§€í‘œ ë²ˆí˜¸ ì¶”ì¶œ í•¨ìˆ˜
def extract_indicator_number(question):
    """ì§ˆë¬¸ì—ì„œ ì§€í‘œ ë²ˆí˜¸ë¥¼ ì¶”ì¶œ (ì˜ˆ: 'ì§€í‘œ 1ë²ˆ', 'í‰ê°€ì§€í‘œ 5ë²ˆ')"""
    patterns = [
        r'ì§€í‘œ\s*(\d+)\s*ë²ˆ',
        r'í‰ê°€ì§€í‘œ\s*(\d+)\s*ë²ˆ',
        r'(\d+)\s*ë²ˆ\s*ì§€í‘œ',
    ]
    for pattern in patterns:
        match = re.search(pattern, question)
        if match:
            return int(match.group(1))
    return None

# í–¥ìƒëœ ë¬¸ì„œ ê²€ìƒ‰ í•¨ìˆ˜
def search_documents(vectordb, question, k=5, search_type="mmr"):
    """
    ì§ˆë¬¸ì— ë§ëŠ” ë¬¸ì„œ ê²€ìƒ‰
    - ì§€í‘œ ë²ˆí˜¸ê°€ ìˆìœ¼ë©´ ìš°ì„  ë©”íƒ€ë°ì´í„° í•„í„°ë§
    - MMR/ìœ ì‚¬ë„ ê²€ìƒ‰ ì‚¬ìš©
    """
    indicator_num = extract_indicator_number(question)

    # ì§€í‘œ ë²ˆí˜¸ê°€ ìˆëŠ” ê²½ìš° ë©”íƒ€ë°ì´í„° ê¸°ë°˜ ê²€ìƒ‰
    if indicator_num:
        # ë©”íƒ€ë°ì´í„° í•„í„°ë¡œ íŠ¹ì • ì§€í‘œ ê²€ìƒ‰
        filter_dict = {"type": "í‰ê°€ì§€í‘œ"}
        retriever = vectordb.as_retriever(
            search_type="similarity",
            search_kwargs={
                "k": k,
                "filter": filter_dict
            }
        )
        docs = retriever.get_relevant_documents(f"ì§€í‘œ {indicator_num}ë²ˆ")
        # ì¶”ê°€ë¡œ ì§€í‘œ ë²ˆí˜¸ê°€ í¬í•¨ëœ ë¬¸ì„œë§Œ í•„í„°ë§
        filtered_docs = [doc for doc in docs if f"{indicator_num}ë²ˆ" in doc.page_content or f"ì§€í‘œ{indicator_num}" in doc.page_content]
        if filtered_docs:
            return filtered_docs[:k]

    # ì¼ë°˜ ê²€ìƒ‰ (MMR ë˜ëŠ” ìœ ì‚¬ë„)
    if search_type == "mmr":
        retriever = vectordb.as_retriever(
            search_type="mmr",
            search_kwargs={
                "k": k,
                "fetch_k": k * 3,
                "lambda_mult": 0.7
            }
        )
    else:
        retriever = vectordb.as_retriever(
            search_type="similarity",
            search_kwargs={"k": k}
        )

    return retriever.get_relevant_documents(question)

# RAG ì²´ì¸ì€ ì‚¬ìš©í•˜ì§€ ì•Šê³  ì§ì ‘ ê²€ìƒ‰ + LLM í˜¸ì¶œ ë°©ì‹ìœ¼ë¡œ ë³€ê²½
def generate_answer(vectordb, question, k=5, search_type="mmr"):
    """
    ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„±
    Returns: (answer, retrieved_docs)
    """
    # ë¬¸ì„œ ê²€ìƒ‰
    docs = search_documents(vectordb, question, k=k, search_type=search_type)

    # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
    context = "\n\n---\n\n".join([
        f"[ë¬¸ì„œ {i+1}] (í˜ì´ì§€: {doc.metadata.get('page', 'N/A')}, ì„¹ì…˜: {doc.metadata.get('section', 'N/A')})\n{doc.page_content}"
        for i, doc in enumerate(docs)
    ])

    # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    prompt_template = """
    ë„ˆëŠ” 2024ë…„ë„ ì¥ê¸°ìš”ì–‘ê¸°ê´€ ì¬ê°€ê¸‰ì—¬ í‰ê°€ë§¤ë‰´ì–¼ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ëŠ” ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

    ë‹¤ìŒ ì§€ì¹¨ì„ ë”°ë¼ ë‹µë³€í•´ì£¼ì„¸ìš”:
    1. ê´€ë ¨ëœ í‰ê°€ì§€í‘œ ë²ˆí˜¸ê°€ ìˆë‹¤ë©´ ëª…ì‹œí•´ì£¼ì„¸ìš” (ì˜ˆ: "ì§€í‘œ 1ë²ˆ ìš´ì˜ê·œì •")
    2. ì ìˆ˜ ë° í‰ê°€ê¸°ì¤€ì„ êµ¬ì²´ì ìœ¼ë¡œ ì œì‹œí•´ì£¼ì„¸ìš”
    3. ì°¸ê³ í•œ ë¬¸ì„œì˜ í˜ì´ì§€ë‚˜ ì„¹ì…˜ ì •ë³´ë¥¼ í‘œì‹œí•´ì£¼ì„¸ìš”
    4. ì •í™•í•˜ê³  ê°ê´€ì ì¸ ì •ë³´ë§Œ ì œê³µí•˜ê³ , í™•ì‹¤í•˜ì§€ ì•Šì€ ê²½ìš° "ë¬¸ì„œì—ì„œ ëª…í™•í•œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë§í•´ì£¼ì„¸ìš”
    5. í‘œ í˜•ì‹ì˜ ì •ë³´ê°€ ìˆë‹¤ë©´ í‘œë¡œ ì •ë¦¬í•´ì„œ ë³´ì—¬ì£¼ì„¸ìš”

    ì§ˆë¬¸: {question}

    ì°¸ê³  ë¬¸ì„œ:
    {context}

    ë‹µë³€:
    """

    llm = ChatOpenAI(
        model="gpt-4o-mini",
        temperature=0.1,
        max_tokens=3000
    )

    prompt = ChatPromptTemplate.from_template(prompt_template)
    messages = prompt.format_messages(question=question, context=context)
    response = llm.invoke(messages)

    return response.content, docs

# Streamlit UI
st.set_page_config(page_title="ì¥ê¸°ìš”ì–‘ ì¬ê°€ê¸‰ì—¬ í‰ê°€ ë©”ë‰´ì–¼", page_icon="ğŸ“˜")
st.title("ğŸ“˜ ì¥ê¸°ìš”ì–‘ ì¬ê°€ê¸‰ì—¬ í‰ê°€ ë©”ë‰´ì–¼ ì±—ë´‡")

# ì±—ë´‡ ì†Œê°œ ë° ì„¤ëª…
with st.expander("â„¹ï¸ ì±—ë´‡ ì‚¬ìš© ì•ˆë‚´", expanded=False):
    st.markdown("""
    ### ğŸ“‹ ì´ ì±—ë´‡ì— ëŒ€í•´

    ì´ ì±—ë´‡ì€ **2024ë…„ë„ ì¥ê¸°ìš”ì–‘ê¸°ê´€ ì¬ê°€ê¸‰ì—¬ í‰ê°€ë§¤ë‰´ì–¼ â…¡ (ì£¼ì•¼ê°„ë³´í˜¸, ë‹¨ê¸°ë³´í˜¸)**ë¥¼ ê¸°ë°˜ìœ¼ë¡œ
    ë‹µë³€ì„ ì œê³µí•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

    ### ğŸ’¡ ì œê³µí•˜ëŠ” ì •ë³´

    - **í‰ê°€ì§€í‘œ ë° í‰ê°€ê¸°ì¤€**: ê° ì§€í‘œë³„ ìƒì„¸ í‰ê°€ ê¸°ì¤€
    - **ì ìˆ˜ êµ¬ì„± ë° ë°°ì **: í‰ê°€ ì²™ë„ ë° ì ìˆ˜ ì‚°ì • ë°©ë²•
    - **2020ë…„ vs 2024ë…„ ë³€ê²½ì‚¬í•­**: ë§¤ë‰´ì–¼ ê°œì • ë‚´ìš© ë¹„êµ
    - **í‰ê°€ì ì¤€ìˆ˜ì‚¬í•­**: í‰ê°€ìê°€ ì§€ì¼œì•¼ í•  ì‚¬í•­
    - **ë§¤ë‰´ì–¼ ì¼ë°˜ì‚¬í•­**: í‰ê°€ ë°©ë²•, ì ìš© ê¸°ê°„ ë“±

    ### ğŸ¯ í™œìš© ì˜ˆì‹œ

    - "ì£¼ì•¼ê°„ë³´í˜¸ í‰ê°€ì§€í‘œ 1ë²ˆì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”"
    - "ê²½ë ¥ì§ í‰ê°€ê¸°ì¤€ì€ ë¬´ì—‡ì¸ê°€ìš”?"
    - "2020ë…„ê³¼ 2024ë…„ ë§¤ë‰´ì–¼ì˜ ì°¨ì´ì ì€?"
    - "ì¸ë ¥ì¶”ê°€ë°°ì¹˜ ê°€ì‚°ì ìˆ˜ ê³„ì‚° ë°©ë²•ì„ ì•Œë ¤ì£¼ì„¸ìš”"
    - "ê±´ê°•ê²€ì§„ ê´€ë ¨ ê·œì •ì´ ë¬´ì—‡ì¸ê°€ìš”?"

    ### âš™ï¸ ì‹œìŠ¤í…œ íŠ¹ì§•

    - **ì •í™•í•œ ê²€ìƒ‰**: MMR(Maximal Marginal Relevance) ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ë‹¤ì–‘í•˜ê³  ê´€ë ¨ì„± ë†’ì€ ì •ë³´ ê²€ìƒ‰
    - **ë©”íƒ€ë°ì´í„° í™œìš©**: ì„¹ì…˜, í˜ì´ì§€, ë¬¸ì„œ íƒ€ì… ì •ë³´ë¥¼ í™œìš©í•œ ì •ë°€ ê²€ìƒ‰
    - **ì¶œì²˜ í‘œì‹œ**: ë‹µë³€ì— ê´€ë ¨ í˜ì´ì§€ ë° ì§€í‘œ ë²ˆí˜¸ í¬í•¨
    - **ê°ê´€ì  ë‹µë³€**: ë¬¸ì„œì— ê¸°ë°˜í•œ ì •í™•í•˜ê³  ê°ê´€ì ì¸ ì •ë³´ë§Œ ì œê³µ

    ### âš ï¸ ì£¼ì˜ì‚¬í•­

    - ì´ ì±—ë´‡ì€ ë§¤ë‰´ì–¼ ë¬¸ì„œ ë‚´ìš©ë§Œì„ ì°¸ì¡°í•˜ì—¬ ë‹µë³€í•©ë‹ˆë‹¤
    - ë²•ì  ìë¬¸ì´ë‚˜ ê³µì‹ ê²°ì •ì„ ëŒ€ì²´í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤
    - ì •í™•í•œ ì •ë³´ëŠ” ê³µì‹ ë§¤ë‰´ì–¼ì„ ì§ì ‘ í™•ì¸í•´ì£¼ì„¸ìš”
    """)

st.divider()

# ê²€ìƒ‰ ì˜µì…˜ ì„¤ì •
with st.sidebar:
    st.header("âš™ï¸ ê²€ìƒ‰ ì„¤ì •")
    search_type = st.radio(
        "ê²€ìƒ‰ ë°©ì‹",
        ["mmr", "similarity"],
        format_func=lambda x: "MMR (ë‹¤ì–‘ì„± ì¤‘ì‹¬)" if x == "mmr" else "ìœ ì‚¬ë„ (ì •í™•ì„± ì¤‘ì‹¬)",
        help="MMR: ë‹¤ì–‘í•œ ì •ë³´ ê²€ìƒ‰, ìœ ì‚¬ë„: ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ì •ë³´ ê²€ìƒ‰"
    )
    k_docs = st.slider(
        "ê²€ìƒ‰í•  ë¬¸ì„œ ê°œìˆ˜",
        min_value=3,
        max_value=10,
        value=5,
        help="ë” ë§ì€ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ë©´ ë” í¬ê´„ì ì¸ ë‹µë³€ì„ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤"
    )
    show_sources = st.checkbox("ê²€ìƒ‰ëœ ë¬¸ì„œ í‘œì‹œ", value=True, help="ë‹µë³€ ìƒì„±ì— ì‚¬ìš©ëœ ì›ë³¸ ë¬¸ì„œë¥¼ í‘œì‹œí•©ë‹ˆë‹¤")

# ë²¡í„° ì €ì¥ì†Œ ë¡œë“œ (ìºì‹œë¡œ í•œ ë²ˆë§Œ ë¡œë“œ)
try:
    vectordb = load_vectorstore()
    st.sidebar.success("âœ… ë¬¸ì„œ ì¤€ë¹„ ì™„ë£Œ!")
except FileNotFoundError as e:
    st.error(str(e))
    st.info("ğŸ’¡ ì‚¬ìš© ë°©ë²•:\n1. í„°ë¯¸ë„ì—ì„œ `python create_embeddings.py` ì‹¤í–‰\n2. ì´ í˜ì´ì§€ ìƒˆë¡œê³ ì¹¨")
    st.stop()

# ì§ˆë¬¸ ì…ë ¥
st.subheader("ğŸ’¬ ì§ˆë¬¸í•˜ê¸°")

# ì˜ˆì‹œ ì§ˆë¬¸ ë²„íŠ¼
st.write("**ë¹ ë¥¸ ì§ˆë¬¸ ì˜ˆì‹œ:**")
col1, col2, col3 = st.columns(3)

with col1:
    if st.button("ğŸ“‹ í‰ê°€ì§€í‘œ 1ë²ˆ", use_container_width=True):
        st.session_state.example_question = "ì£¼ì•¼ê°„ë³´í˜¸ í‰ê°€ì§€í‘œ 1ë²ˆ ìš´ì˜ê·œì •ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”"
with col2:
    if st.button("ğŸ‘¥ ê²½ë ¥ì§ ê¸°ì¤€", use_container_width=True):
        st.session_state.example_question = "ê²½ë ¥ì§ í‰ê°€ê¸°ì¤€ì€ ë¬´ì—‡ì¸ê°€ìš”?"
with col3:
    if st.button("ğŸ“Š 2020 vs 2024", use_container_width=True):
        st.session_state.example_question = "2020ë…„ê³¼ 2024ë…„ ë§¤ë‰´ì–¼ì˜ ì£¼ìš” ì°¨ì´ì ì€ ë¬´ì—‡ì¸ê°€ìš”?"

# ì§ˆë¬¸ ì…ë ¥ì°½
if "example_question" in st.session_state:
    question = st.text_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:", value=st.session_state.example_question)
    del st.session_state.example_question
else:
    question = st.text_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:", placeholder="ì˜ˆ: ê±´ê°•ê²€ì§„ ê´€ë ¨ ê·œì •ì´ ë¬´ì—‡ì¸ê°€ìš”?")

# ë‹µë³€ ìƒì„±
if question:
    # ì§€í‘œ ë²ˆí˜¸ ê°ì§€ í‘œì‹œ
    indicator_num = extract_indicator_number(question)
    if indicator_num:
        st.info(f"ğŸ¯ ì§€í‘œ {indicator_num}ë²ˆì— ëŒ€í•œ ì§ˆë¬¸ìœ¼ë¡œ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤. ê´€ë ¨ ë¬¸ì„œë¥¼ ìš°ì„  ê²€ìƒ‰í•©ë‹ˆë‹¤.")

    with st.spinner("ğŸ” ê´€ë ¨ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ê³  ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘..."):
        answer, retrieved_docs = generate_answer(
            vectordb,
            question,
            k=k_docs,
            search_type=search_type
        )

        st.success("âœ… ë‹µë³€ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤!")
        st.write("### ğŸ“ ë‹µë³€:")
        st.write(answer)

        # ê²€ìƒ‰ëœ ë¬¸ì„œ í‘œì‹œ
        if show_sources and retrieved_docs:
            st.divider()
            st.subheader("ğŸ“š ê²€ìƒ‰ëœ ì›ë³¸ ë¬¸ì„œ")
            st.caption(f"ì´ {len(retrieved_docs)}ê°œì˜ ë¬¸ì„œê°€ ê²€ìƒ‰ë˜ì—ˆìŠµë‹ˆë‹¤.")

            for i, doc in enumerate(retrieved_docs):
                with st.expander(f"ğŸ“„ ë¬¸ì„œ {i+1} - í˜ì´ì§€ {doc.metadata.get('page', 'N/A')} ({doc.metadata.get('section', 'ì„¹ì…˜ ë¯¸ë¶„ë¥˜')})"):
                    st.markdown(f"**ë©”íƒ€ë°ì´í„°:**")
                    st.json({
                        "í˜ì´ì§€": doc.metadata.get('page', 'N/A'),
                        "ì„¹ì…˜": doc.metadata.get('section', 'ë¯¸ë¶„ë¥˜'),
                        "íƒ€ì…": doc.metadata.get('type', 'ë¯¸ë¶„ë¥˜')
                    })
                    st.markdown(f"**ë‚´ìš©:**")
                    st.text_area(
                        f"ë‚´ìš© {i+1}",
                        value=doc.page_content,
                        height=200,
                        key=f"doc_{i}",
                        label_visibility="collapsed"
                    )

        # ê²€ìƒ‰ ë°©ì‹ ì •ë³´
        with st.expander("â„¹ï¸ ê²€ìƒ‰ ì„¤ì • ì •ë³´"):
            st.info(f"""
            **í˜„ì¬ ê²€ìƒ‰ ì„¤ì •**
            - ê²€ìƒ‰ ë°©ì‹: {'MMR (ë‹¤ì–‘ì„± ì¤‘ì‹¬)' if search_type == 'mmr' else 'ìœ ì‚¬ë„ (ì •í™•ì„± ì¤‘ì‹¬)'}
            - ê²€ìƒ‰ ë¬¸ì„œ ê°œìˆ˜: {k_docs}ê°œ
            - ì§€í‘œ ë²ˆí˜¸ ê°ì§€: {'ì˜ˆ (ì§€í‘œ ' + str(indicator_num) + 'ë²ˆ)' if indicator_num else 'ì•„ë‹ˆì˜¤'}
            - LLM ëª¨ë¸: GPT-4o-mini
            """)
