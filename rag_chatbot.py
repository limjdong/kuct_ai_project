import streamlit as st
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import OpenAIEmbeddings
from langchain_community.chat_models import ChatOpenAI
from langchain_core.runnables import RunnableLambda
from langchain_core.prompts import ChatPromptTemplate
from dotenv import load_dotenv
import os

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv("env.txt")

# ë²¡í„° ì €ì¥ì†Œ ë¡œë“œ (ì‚¬ì „ ìƒì„±ëœ ì„ë² ë”© ì‚¬ìš©)
def load_vectorstore(vectorstore_path="vectorstore"):
    """ì‚¬ì „ ìƒì„±ëœ FAISS ë²¡í„° ì €ì¥ì†Œ ë¡œë“œ"""
    if not os.path.exists(vectorstore_path):
        raise FileNotFoundError(
            f"ë²¡í„° ì €ì¥ì†Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {vectorstore_path}\n"
            "ë¨¼ì € 'python create_embeddings.py'ë¥¼ ì‹¤í–‰í•˜ì—¬ ì„ë² ë”©ì„ ìƒì„±í•˜ì„¸ìš”."
        )
    embeddings = OpenAIEmbeddings()
    return FAISS.load_local(vectorstore_path, embeddings, allow_dangerous_deserialization=True)

# RAG ì²´ì¸ êµ¬ì„±
def build_rag_chain(vectordb):
    # MMR ë°©ì‹ìœ¼ë¡œ ê²€ìƒ‰ ë‹¤ì–‘ì„± í™•ë³´ + k=4ë¡œ ì¦ê°€
    retriever = vectordb.as_retriever(
        search_type="mmr",  # Maximal Marginal Relevance
        search_kwargs={
            "k": 4,  # ìµœì¢… ë°˜í™˜ ë¬¸ì„œ ìˆ˜
            "fetch_k": 10,  # í›„ë³´ ë¬¸ì„œ ìˆ˜
            "lambda_mult": 0.7  # ìœ ì‚¬ë„(1.0)ì™€ ë‹¤ì–‘ì„±(0.0) ê· í˜•
        }
    )

    # ê°œì„ ëœ í”„ë¡¬í”„íŠ¸: ë” êµ¬ì²´ì ì¸ ë‹µë³€ ìœ ë„
    prompt = ChatPromptTemplate.from_template(
        """
        ë„ˆëŠ” 2024ë…„ë„ ì¥ê¸°ìš”ì–‘ê¸°ê´€ ì¬ê°€ê¸‰ì—¬ í‰ê°€ë§¤ë‰´ì–¼ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ëŠ” ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

        ë‹¤ìŒ ì§€ì¹¨ì„ ë”°ë¼ ë‹µë³€í•´ì£¼ì„¸ìš”:
        1. ê´€ë ¨ëœ í‰ê°€ì§€í‘œ ë²ˆí˜¸ê°€ ìˆë‹¤ë©´ ëª…ì‹œí•´ì£¼ì„¸ìš” (ì˜ˆ: "ì§€í‘œ 1ë²ˆ ìš´ì˜ê·œì •")
        2. ì ìˆ˜ ë° í‰ê°€ê¸°ì¤€ì„ êµ¬ì²´ì ìœ¼ë¡œ ì œì‹œí•´ì£¼ì„¸ìš”
        3. ì°¸ê³ í•œ ë¬¸ì„œì˜ í˜ì´ì§€ë‚˜ ì„¹ì…˜ ì •ë³´ë¥¼ í‘œì‹œí•´ì£¼ì„¸ìš”
        4. ì •í™•í•˜ê³  ê°ê´€ì ì¸ ì •ë³´ë§Œ ì œê³µí•˜ê³ , í™•ì‹¤í•˜ì§€ ì•Šì€ ê²½ìš° "ë¬¸ì„œì—ì„œ ëª…í™•í•œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë§í•´ì£¼ì„¸ìš”

        ì§ˆë¬¸: {question}

        ì°¸ê³  ë¬¸ì„œ:
        {context}

        ë‹µë³€:
        """
    )

    llm = ChatOpenAI(
        model="gpt-4o-mini",
        temperature=0.1,  # ì•½ê°„ì˜ ì°½ì˜ì„± í—ˆìš©
        max_tokens=3000  # ë” ìƒì„¸í•œ ë‹µë³€ ê°€ëŠ¥
    )

    rag_chain = (
        {
            "context": RunnableLambda(lambda x: x["question"]) | retriever,
            "question": RunnableLambda(lambda x: x["question"])
        }
        | prompt
        | llm
    )
    return rag_chain

# Streamlit UI
st.set_page_config(page_title="ì¥ê¸°ìš”ì–‘ ì¬ê°€ê¸‰ì—¬ í‰ê°€ ë©”ë‰´ì–¼", page_icon="ğŸ“˜")
st.title("ğŸ“˜ ì¥ê¸°ìš”ì–‘ ì¬ê°€ê¸‰ì—¬ í‰ê°€ ë©”ë‰´ì–¼ ì±—ë´‡")

# ì±—ë´‡ ì†Œê°œ ë° ì„¤ëª…
with st.expander("â„¹ï¸ ì±—ë´‡ ì‚¬ìš© ì•ˆë‚´", expanded=False):
    st.markdown("""
    ### ğŸ“‹ ì´ ì±—ë´‡ì— ëŒ€í•´

    ì´ ì±—ë´‡ì€ **2024ë…„ë„ ì¥ê¸°ìš”ì–‘ê¸°ê´€ ì¬ê°€ê¸‰ì—¬ í‰ê°€ë§¤ë‰´ì–¼ â…¡ (ì£¼ì•¼ê°„ë³´í˜¸, ë‹¨ê¸°ë³´í˜¸)**ë¥¼ ê¸°ë°˜ìœ¼ë¡œ
    ë‹µë³€ì„ ì œê³µí•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

    ### ğŸ’¡ ì œê³µí•˜ëŠ” ì •ë³´

    - **í‰ê°€ì§€í‘œ ë° í‰ê°€ê¸°ì¤€**: ê° ì§€í‘œë³„ ìƒì„¸ í‰ê°€ ê¸°ì¤€
    - **ì ìˆ˜ êµ¬ì„± ë° ë°°ì **: í‰ê°€ ì²™ë„ ë° ì ìˆ˜ ì‚°ì • ë°©ë²•
    - **2020ë…„ vs 2024ë…„ ë³€ê²½ì‚¬í•­**: ë§¤ë‰´ì–¼ ê°œì • ë‚´ìš© ë¹„êµ
    - **í‰ê°€ì ì¤€ìˆ˜ì‚¬í•­**: í‰ê°€ìê°€ ì§€ì¼œì•¼ í•  ì‚¬í•­
    - **ë§¤ë‰´ì–¼ ì¼ë°˜ì‚¬í•­**: í‰ê°€ ë°©ë²•, ì ìš© ê¸°ê°„ ë“±

    ### ğŸ¯ í™œìš© ì˜ˆì‹œ

    - "ì£¼ì•¼ê°„ë³´í˜¸ í‰ê°€ì§€í‘œ 1ë²ˆì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”"
    - "ê²½ë ¥ì§ í‰ê°€ê¸°ì¤€ì€ ë¬´ì—‡ì¸ê°€ìš”?"
    - "2020ë…„ê³¼ 2024ë…„ ë§¤ë‰´ì–¼ì˜ ì°¨ì´ì ì€?"
    - "ì¸ë ¥ì¶”ê°€ë°°ì¹˜ ê°€ì‚°ì ìˆ˜ ê³„ì‚° ë°©ë²•ì„ ì•Œë ¤ì£¼ì„¸ìš”"
    - "ê±´ê°•ê²€ì§„ ê´€ë ¨ ê·œì •ì´ ë¬´ì—‡ì¸ê°€ìš”?"

    ### âš™ï¸ ì‹œìŠ¤í…œ íŠ¹ì§•

    - **ì •í™•í•œ ê²€ìƒ‰**: MMR(Maximal Marginal Relevance) ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ë‹¤ì–‘í•˜ê³  ê´€ë ¨ì„± ë†’ì€ ì •ë³´ ê²€ìƒ‰
    - **ë©”íƒ€ë°ì´í„° í™œìš©**: ì„¹ì…˜, í˜ì´ì§€, ë¬¸ì„œ íƒ€ì… ì •ë³´ë¥¼ í™œìš©í•œ ì •ë°€ ê²€ìƒ‰
    - **ì¶œì²˜ í‘œì‹œ**: ë‹µë³€ì— ê´€ë ¨ í˜ì´ì§€ ë° ì§€í‘œ ë²ˆí˜¸ í¬í•¨
    - **ê°ê´€ì  ë‹µë³€**: ë¬¸ì„œì— ê¸°ë°˜í•œ ì •í™•í•˜ê³  ê°ê´€ì ì¸ ì •ë³´ë§Œ ì œê³µ

    ### âš ï¸ ì£¼ì˜ì‚¬í•­

    - ì´ ì±—ë´‡ì€ ë§¤ë‰´ì–¼ ë¬¸ì„œ ë‚´ìš©ë§Œì„ ì°¸ì¡°í•˜ì—¬ ë‹µë³€í•©ë‹ˆë‹¤
    - ë²•ì  ìë¬¸ì´ë‚˜ ê³µì‹ ê²°ì •ì„ ëŒ€ì²´í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤
    - ì •í™•í•œ ì •ë³´ëŠ” ê³µì‹ ë§¤ë‰´ì–¼ì„ ì§ì ‘ í™•ì¸í•´ì£¼ì„¸ìš”
    """)

st.divider()

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "vectordb" not in st.session_state:
    st.session_state.vectordb = None
if "rag_chain" not in st.session_state:
    st.session_state.rag_chain = None

# ë²¡í„° ì €ì¥ì†Œ ë¡œë“œ (ì•± ì‹œì‘ ì‹œ í•œ ë²ˆë§Œ)
if st.session_state.vectordb is None:
    try:
        with st.spinner("ë²¡í„° ì €ì¥ì†Œ ë¡œë”© ì¤‘..."):
            st.session_state.vectordb = load_vectorstore()
            st.session_state.rag_chain = build_rag_chain(st.session_state.vectordb)
            st.success("âœ… ë¬¸ì„œ ì¤€ë¹„ ì™„ë£Œ! ì§ˆë¬¸ì„ ì…ë ¥í•´ë³´ì„¸ìš”.")
    except FileNotFoundError as e:
        st.error(str(e))
        st.info("ğŸ’¡ ì‚¬ìš© ë°©ë²•:\n1. í„°ë¯¸ë„ì—ì„œ `python create_embeddings.py` ì‹¤í–‰\n2. ì´ í˜ì´ì§€ ìƒˆë¡œê³ ì¹¨")
        st.stop()

# ì§ˆë¬¸ ì…ë ¥
st.subheader("ğŸ’¬ ì§ˆë¬¸í•˜ê¸°")

# ì˜ˆì‹œ ì§ˆë¬¸ ë²„íŠ¼
st.write("**ë¹ ë¥¸ ì§ˆë¬¸ ì˜ˆì‹œ:**")
col1, col2, col3 = st.columns(3)

with col1:
    if st.button("ğŸ“‹ í‰ê°€ì§€í‘œ 1ë²ˆ", use_container_width=True):
        st.session_state.example_question = "ì£¼ì•¼ê°„ë³´í˜¸ í‰ê°€ì§€í‘œ 1ë²ˆ ìš´ì˜ê·œì •ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”"
with col2:
    if st.button("ğŸ‘¥ ê²½ë ¥ì§ ê¸°ì¤€", use_container_width=True):
        st.session_state.example_question = "ê²½ë ¥ì§ í‰ê°€ê¸°ì¤€ì€ ë¬´ì—‡ì¸ê°€ìš”?"
with col3:
    if st.button("ğŸ“Š 2020 vs 2024", use_container_width=True):
        st.session_state.example_question = "2020ë…„ê³¼ 2024ë…„ ë§¤ë‰´ì–¼ì˜ ì£¼ìš” ì°¨ì´ì ì€ ë¬´ì—‡ì¸ê°€ìš”?"

# ì§ˆë¬¸ ì…ë ¥ì°½
if "example_question" in st.session_state:
    question = st.text_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:", value=st.session_state.example_question)
    del st.session_state.example_question
else:
    question = st.text_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:", placeholder="ì˜ˆ: ê±´ê°•ê²€ì§„ ê´€ë ¨ ê·œì •ì´ ë¬´ì—‡ì¸ê°€ìš”?")

# ë‹µë³€ ìƒì„±
if question and st.session_state.rag_chain:
    with st.spinner("ğŸ” ê´€ë ¨ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ê³  ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘..."):
        result = st.session_state.rag_chain.invoke(
            {"question": question},
        )
        st.success("âœ… ë‹µë³€ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤!")
        st.write("### ğŸ“ ë‹µë³€:")
        st.write(result.content)

        # ì°¸ê³  ì •ë³´
        with st.expander("ğŸ“Œ ì°¸ê³  ì •ë³´"):
            st.info("""
            **ë‹µë³€ ìƒì„± ë°©ì‹**
            - MMR ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ 4ê°œì˜ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
            - GPT-4o-mini ëª¨ë¸ë¡œ ë‹µë³€ ìƒì„±
            - ë©”íƒ€ë°ì´í„°(ì„¹ì…˜, í˜ì´ì§€)ë¥¼ í™œìš©í•œ ì •í™•í•œ ì¶œì²˜ ì œê³µ
            """)
