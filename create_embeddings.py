"""
PDF ë¬¸ì„œë¥¼ ì„ë² ë”©í•˜ì—¬ FAISS ë²¡í„° ì €ì¥ì†Œë¡œ ì €ì¥í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
"""
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import OpenAIEmbeddings
from dotenv import load_dotenv
import os
import json
import re

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv("env.txt")

def extract_indicator_numbers(text):
    """í…ìŠ¤íŠ¸ì—ì„œ ì§€í‘œ ë²ˆí˜¸ë¥¼ ì¶”ì¶œ"""
    patterns = [
        r'ì§€í‘œ\s*(\d+)',
        r'í‰ê°€ì§€í‘œ\s*(\d+)',
    ]
    numbers = set()
    for pattern in patterns:
        matches = re.findall(pattern, text)
        numbers.update([int(m) for m in matches])
    return list(numbers)

def create_and_save_embeddings(pdf_path, save_dir="vectorstore"):
    """
    PDF íŒŒì¼ì„ ë¡œë“œí•˜ì—¬ ì„ë² ë”©ì„ ìƒì„±í•˜ê³  ì €ì¥

    Args:
        pdf_path: PDF íŒŒì¼ ê²½ë¡œ
        save_dir: ë²¡í„° ì €ì¥ì†Œë¥¼ ì €ì¥í•  ë””ë ‰í† ë¦¬
    """
    print(f"ğŸ“– PDF ë¡œë“œ ì¤‘: {pdf_path}")
    loader = PyPDFLoader(pdf_path)
    documents = loader.load()

    print(f"ğŸ“„ ì´ {len(documents)} í˜ì´ì§€ ë¡œë“œë¨")

    # ë©”íƒ€ë°ì´í„° ë³´ê°• - ì„¹ì…˜ ë° í˜ì´ì§€ ì •ë³´ ì¶”ê°€
    print("ğŸ“‹ ë©”íƒ€ë°ì´í„° ë³´ê°• ì¤‘...")
    for i, doc in enumerate(documents):
        doc.metadata['page'] = i + 1

        # ì„¹ì…˜ ì •ë³´ ì¶”ì¶œ
        content = doc.page_content
        if "ì£¼ì•¼ê°„ë³´í˜¸" in content:
            doc.metadata['section'] = "ì£¼ì•¼ê°„ë³´í˜¸"
        elif "ë‹¨ê¸°ë³´í˜¸" in content:
            doc.metadata['section'] = "ë‹¨ê¸°ë³´í˜¸"
        elif "í‰ê°€ì ì¤€ìˆ˜ì‚¬í•­" in content or "í‰ê°€ìì˜ ê¸°ë³¸ìì„¸" in content:
            doc.metadata['section'] = "í‰ê°€ìì¤€ìˆ˜ì‚¬í•­"
        elif "ë§¤ë‰´ì–¼ ì¼ë°˜ì‚¬í•­" in content:
            doc.metadata['section'] = "ì¼ë°˜ì‚¬í•­"

        # ë¬¸ì„œ íƒ€ì… ì •ë³´ ì¶”ì¶œ
        if "í‰ê°€ì§€í‘œ" in content:
            doc.metadata['type'] = "í‰ê°€ì§€í‘œ"
        elif "í‰ê°€ê¸°ì¤€" in content:
            doc.metadata['type'] = "í‰ê°€ê¸°ì¤€"
        elif "í™•ì¸ë°©ë²•" in content:
            doc.metadata['type'] = "í™•ì¸ë°©ë²•"

    print("âœ‚ï¸  ë¬¸ì„œ ë¶„í•  ì¤‘...")
    # ì²­í‚¹ ìµœì í™”: chunk_size 800, overlap 150ìœ¼ë¡œ ì¡°ì •
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=800,
        chunk_overlap=150,
        separators=["\n\n", "\n", "ã€‚", " ", ""],
        length_function=len,
    )
    split_docs = splitter.split_documents(documents)

    print(f"ğŸ“ ì´ {len(split_docs)} ê°œì˜ ì²­í¬ë¡œ ë¶„í• ë¨ (ìµœì í™”ëœ í¬ê¸°)")

    print("ğŸ”„ ì„ë² ë”© ìƒì„± ì¤‘... (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
    embeddings = OpenAIEmbeddings()

    # ë°°ì¹˜ ì²˜ë¦¬ë¡œ í† í° ì œí•œ íšŒí”¼ (í•œ ë²ˆì— 100ê°œì”© ì²˜ë¦¬)
    batch_size = 100
    total_batches = (len(split_docs) - 1) // batch_size + 1

    # ì²« ë²ˆì§¸ ë°°ì¹˜ë¡œ ë²¡í„°ìŠ¤í† ì–´ ì´ˆê¸°í™”
    first_batch = split_docs[0:batch_size]
    print(f"  â†’ ë°°ì¹˜ 1/{total_batches} ì²˜ë¦¬ ì¤‘... ({len(first_batch)}ê°œ ì²­í¬)")
    vectorstore = FAISS.from_documents(first_batch, embeddings)

    # ë‚˜ë¨¸ì§€ ë°°ì¹˜ ì²˜ë¦¬
    for i in range(batch_size, len(split_docs), batch_size):
        batch = split_docs[i:i + batch_size]
        batch_num = i // batch_size + 1
        print(f"  â†’ ë°°ì¹˜ {batch_num}/{total_batches} ì²˜ë¦¬ ì¤‘... ({len(batch)}ê°œ ì²­í¬)")

        temp_vectorstore = FAISS.from_documents(batch, embeddings)
        vectorstore.merge_from(temp_vectorstore)

    print(f"ğŸ’¾ ë²¡í„° ì €ì¥ì†Œ ì €ì¥ ì¤‘: {save_dir}")
    os.makedirs(save_dir, exist_ok=True)
    vectorstore.save_local(save_dir)

    # ì§€í‘œ ë²ˆí˜¸ë³„ ë§¤í•‘ ìƒì„±
    print("ğŸ”¢ ì§€í‘œ ë²ˆí˜¸ ë§¤í•‘ ìƒì„± ì¤‘...")
    indicator_mapping = {}

    for idx, doc in enumerate(split_docs):
        # ë¬¸ì„œ ë‚´ìš©ì—ì„œ ì§€í‘œ ë²ˆí˜¸ ì¶”ì¶œ
        indicator_nums = extract_indicator_numbers(doc.page_content)

        for num in indicator_nums:
            if num not in indicator_mapping:
                indicator_mapping[num] = {
                    'chunk_indices': [],
                    'chunks': []
                }
            indicator_mapping[num]['chunk_indices'].append(idx)
            # ì²­í¬ ì •ë³´ ì €ì¥ (ë‚´ìš©, ë©”íƒ€ë°ì´í„°)
            indicator_mapping[num]['chunks'].append({
                'content': doc.page_content,
                'metadata': {
                    'page': doc.metadata.get('page', 'N/A'),
                    'section': doc.metadata.get('section', 'N/A'),
                    'type': doc.metadata.get('type', 'N/A')
                }
            })

    # JSONìœ¼ë¡œ ì €ì¥
    mapping_file = os.path.join(save_dir, "indicator_mapping.json")
    with open(mapping_file, 'w', encoding='utf-8') as f:
        json.dump(indicator_mapping, f, ensure_ascii=False, indent=2)

    print(f"ğŸ“Š ì§€í‘œ ë§¤í•‘ ì €ì¥ ì™„ë£Œ: {mapping_file}")
    print(f"   ì´ {len(indicator_mapping)}ê°œì˜ ì§€í‘œ ë²ˆí˜¸ê°€ ë§¤í•‘ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print(f"   ë§¤í•‘ëœ ì§€í‘œ ë²ˆí˜¸: {sorted(indicator_mapping.keys())}")

    print("âœ… ì™„ë£Œ! ë©”íƒ€ë°ì´í„°ê°€ í¬í•¨ëœ ìµœì í™”ëœ ì„ë² ë”©ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print(f"   ì´ {len(split_docs)}ê°œì˜ ì²­í¬ê°€ ì„ë² ë”©ë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    # manual.pdf íŒŒì¼ ì„ë² ë”© ìƒì„± (í…ŒìŠ¤íŠ¸ìš©)
    pdf_file = "manual.pdf"

    if not os.path.exists(pdf_file):
        print(f"âŒ ì˜¤ë¥˜: {pdf_file} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("ğŸ’¡ manual.pdfë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤...")
        pdf_file = "manual.pdf"

    if not os.path.exists(pdf_file):
        print(f"âŒ ì˜¤ë¥˜: PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        exit(1)

    create_and_save_embeddings(pdf_file)
